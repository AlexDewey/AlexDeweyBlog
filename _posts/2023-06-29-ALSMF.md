---
title: "Why Are Worse Algorithms Better?"
date: 2023-06-29
---

# When you're a hammer, everything looks like a nail. #

We often overplay the importance of newer and shinnier algorithms compared to older and forgotten techniques in machine learning. Two papers recently have stood out and demonstrated this better than anything else.

1. Think back to the beginnings of Netflix. Their recommendation system had an RMSE (error) of .9513, and would give a million dollars to the first team to lower that by 10%. It took about three years until we got the prize. All that was used to do this for this time was Matrix Factorization, and the better understanding of how to implement and use the algorithm.

"On the Difficulty of Evaluating Baselines" by Rendle et al. is an incredibly eye-opening look into how improper baselines in the development of recommendation systems leads to the inflated results of papers. Over the years researchers put out models that were acrewing better and better results. These results were ficticious, as the preprocessing and handling of data resulted in better results rather than the techniques themselves. The authors argue that hyperparameters and the handling of data makes a massive difference in the reported value, and I can definitely attest to this.

In my own recreation of a paper called "Neural Collaborative Filtering", the results of the paper indicated that the method would have an HR@10 of 68%, but my version only had 17%! In reading the paper closer to understand what went wrong, a single line described how the authors bumped up their results by removing a large section of data to choose from, making the problem much easier to solve and justified this practice by referencing two other papers. "Since it is too time consuming to rank all items for every user during evaluation, we followed the common strategy [6, 21] that randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items." After reading this, I added this augment in the problem and found my 17% jumped to 53%! This still wasn't the 68% claimed, as I had differences in handling data and different hyperparameters that drastically altered the final result.

The paper talks about my issue of accuracy discrepency and calls for a rigid handling of not only the dataset, but a standardized protocol of how data is handled and how hyperparameters are found. A rigid protocol may mean that specific methods may "overfit", however the authors discuss that the three year endeavor to lower the Netflix Prize didn't result in methods that overfit, but gave valuable lessons in properly tailing an algorithm to a dataset. "While signs of overfitting might show up in the long run, the benefits from well calibrated results outweight issues that improper baselines might cause."

So what were the results of this paper when these Google Researchers modified matrix factorization on a problem that had far passed such basic means? The most recent time was AdaError with 0.7644. Remember that 0.01 of a difference is a huge margin, especially during the times of the Netflix Prize.

Stochastic gradient descent matrix factorization (SGD-MF) had 0.7720. Then, they trained a bayesian matrix factorization model (BMF) using gibbs sampling which got 0.7633, already beating the best algorithm. Then they added a time variable to their BMF and got 0.7587. They trained a cheap bag-of-words predictor to see which words were used in movies and this resulted in 0.7563. They continue adding small benefits until they arrive at a final 0.7485 RMSE, by far beating the most recent model.

2. "A ConvNet for the 2020s" by Liu et al. is another example of poor experimental design. As transformers were growing in popularity, the optimization of existing technology fell by the way side. The best transformer at the time Swin-L achieved a top-1 accuracy of 87.3%, while the optimized ConvNeXt-XL model achieved 87.8%. Not only was the ConvNeXt model better, but simpler and faster.

They changed the stage compute ratio, changed the stem to "patchify", had convolutional filters separated into different groups, inverted the bottleneck, used larger kernel sizes, used GELU instead of ReLu, used fewer activations functions and normalization layers, substituted batch normalization with layer normalization and separated downsampling layers.

This paper was written in March 2022, and of course because of the rapid acceleration we've jumped from 87.8% to 91.1% as of June 2023.


# The picture I'm painting here is probably misleading. #

Newer models such as transformers are better. Basic bayesian matrix factorization with some added bonuses will not compete against Amazon's much more advanced graphical neural net recommendation systems. The purpose is to 
