---
title: "The Theoretical Underpinnings to Analysis of Modern Multimodal Generative Latent Models"
date: 2024-03-22
---

"How do we take in complicated data and accurately generate more of it?"

What motivates this post is an interest and desire to develop a keen understanding of how deep generative models work. From a
theoretical to a practical analysis I want to be able to understand and explain these collections of models. This field is massive,
and for my purposes I'll be examining and walking through latent models. Latent models specialize in their ability to understand
complex sorces of data. My research involves the use of multimodal data to improve performance of generative models, so with this
focus we can begin our adventure!

The <b>manifold hypothesis</b> states that all real-world expressions lie upon a smaller subset of data, that
expresses itself in a higher dimensionality. This hypothesis underlies much of how machine learning works, and
intuitively makes sense. When we imagine objects, our brain stores said objects in a collection of neurons, there exists
this compression of information.

Let's say we have a highly complex piece of data x, say a picture of a horse. We wish to compress said horse into a manifold.
The representation that horse resides in is our <b>latent space</b> denoted as z.

What would the process be for not only learning how to represent a horse (autoencoder), but to also represent a <b>distribution</b>
of horses? We want to represent horses, as well as all kinds of horses in a statistical manner. I find a lot of talk
around this topic does the ground work of math first, and builds up to the bigger picture. I'm going to instead describe
the bigger picture, and work my way down to the math such that we cultivate a better intuitive feeling for the math.

[img]

1. We want to encode some x into a latent space z. The probability density function (pdf) of this is "p(z|x)". (A pdf is just a function of probability over a space. So over a discrete space of a coin flip, tails is 0.5 and heads is 0.5. Over a continuous space, we find that calculating the probability at any given space is hard, unless we're dealing with very specific functions be it gaussian or poisson distributions.)
2. We want to sample from said latent space z, such that we get a sampling away from out simple sample. The pdf of this is "p(z)".
3. We want to decode that sample into a new x'. The pdf of this is "p(x'|z)".
4. We wnat to tweak our encoding and decoding process to not only represent x', but to represent the distribution that x' comes from.

Now... 

1. Encode a value to a mean and covariance matrix: $$x \rightarrow \mu(x), \Sigma(x)$$
2. Calculate a sample: $$z = \mu(x) + \Sigma(x) \cdot \epsilon \text{, where } \epsilon \sim \mathcal{N}(0, 1)$$
3. Decode $$z \rightarrow p(x|z) $$
4. Optimize the process, taking into consideration how well the VAE can create horses, as well as deviate from the mean horse in a proper manner. We are optimizing some parameters given an input x and a chosen prior z.

Let's focus on this last step and how we derive a suitable formula. We know our final optimization must take into
consideration both reconstruction and statistical representation, and we know we can only do this using values we have.
Theoretical values we don't have, for instance, is the perfect encoding p(x|z). We don't know what the ideal distribution
is p(z) in a latent space and instead resort to using a gaussian distribution N(0, 1). So let's start from the beginning
and finally derive this equation and see why it makes sense!

The below equation is our starting point that describes how the probability of x is the probability of x given all
distributions z. Understanding what the actual latent distribution is for z isn't <b>tractable</b>, or isn't really easily
calculatable. Instead we'll come up with neat substitutions down the road!

$$ p(x) = \int p(x|z)d(z)dz $$

Let's first multiply each side by the natural log, as to simplify our calculation eventually to a summation.

$$ ln(p(x)) = ln \int p(x|z)p(z)dz $$

We also know that we don't have the exact distribution p(z) that perfectly maps onto our latent space, so we'll instead use
a tractable distribution q(z). This borrows from <b>variational bayesian methods</b> which look to provide an approximation
of a previously intractable distribution. For now, just imagine we'll sloppily use the normal distribution N(0, 1) for q(z).

$$ ln(p(x)) = ln \int \frac{q(z)}{q(z)} p(x|z)p(z)dz $$

$$ ln(p(x)) = ln \mathbb{E}_{q(z|x)}[\frac{p(x|z)p(z)}{q(z)}] \text{, where }\ \mathbb{E} _{q(z|x)} = \int f(z)q(z)dz $$

We have to move this natural log inside of our expectation, which we can only do with <b>Jensen's Inequality</b>. Jensen's
Inequality is used in this case to show that we cannot perfectly optimize this problem, but rather come to an approximation.

[img of jensen's inequality]

Jensen's inequality does mean that we lose some specificity in our approximations, however consider that our optimization
landscape may actually become easier and less noisy, and there's research to show that getting as close to the lower bound of 
error in estimating our distributions may not actually give us the best results.

$$ ln(p(x)) \geq \mathbb{E}_{q(z|x)}[\frac{p(x|z)p(z)}{q(z)}] $$

$$ ln(p(x)) \geq \mathbb{E}_{q(z|x)}[ln(p(x|z)) + ln(p(z)) - ln(q(z))]$$

$$ ln(p(x)) \geq \mathbb{E} _{q(z|x)}[ln(p(x|z))] - \mathbb{E} _{q(z|x)}[ln((q(z)) - ln(p(z))]$$

This equation is known as our Estimated Lower BOund (<b>ELBO</b>). Let's take a step back from this complex formula that isn't
intuitive and think about what exactly we'd want in a formula for generating statistically representative samples. Well, we'd
want a term to punish how accurate the final result. The second term we'd want is one that enforced the statistical
distribution we're looking for. We're basically desiring a mix between complete recreation accuracy and the variation that
we desire in a generative model. This is exactly what we end up with! Let's take a look.

Our first term is the log likelihood of how well our decoder recreates our data x from our latent data z. Our second term is the 
<b>Kullback Leibler Divergence (KLD)</b> equation, in which the statistical distance between one probability distribution from another. Our two distribution is the 
one we create compared to the one we desired.

$$ \mathbb{E} _{q(z|x)}[ln((q(z)) - ln(p(z))] = \text{D} _{\text{KL}}(P\parallel Q) = \sum _{Z \epsilon z} q(z) log(\frac{q(z)}{p(z)})$$

Here we're summing up the ratios multiplied by q(z) to find the surprise of how close our distributions align. This way we can
enforce how well our model fits the distribution!

$$ \text{Log Loss} \geq \text{Reconstruction Error} + \text{Statistical Distribution Error} $$

The log of p(x|z) we take the natural log of the pdf of a gaussian distribution

$$ \text{Reconstruction Error} = \text{MSE}(x, x') = \sum^{n}_{i=1}\frac{(x _{i} - x' _{i})^{2}}{n} $$

To find out the KLD equation for easily comparing a diagonal multivariate normal to a standard
normal distribution I looked it up on wikipedia and found this equation haha.

$$ \text{Statistical Distribution Error} = D_{KL}(\mathcal{N}(\mu(x), \sigma(x)) || \mathcal{N}(0, I)) = \frac{1}{2} \sum^{k} _{i=1}(\sigma^{2} _{i} + \mu^{2} _{i} - 1 - \text{ln}(\sigma^{2} _{i}))$$

In our code now, we'll take an image x, encode it to two separate vectors &mu; and &sigma; in our latent space. We'll sample from said latent space z = &mu; + 
&sigma; * N(0, I). From this z, we pass it into our decoder and get x'. Then we plug in our values (x, x', &mu;, &sigma;) into our equation and get a loss to then 
optimize our parameters for.


