---
title: "The Theoretical Underpinnings to Analysis of Modern Multimodal Generative Latent Models"
date: 2024-03-22
---

What motivates this post is an interest and desire to develop a keen understanding of how deep generative models work. From a
theoretical to a practical analysis I want to be able to understand and explain these collections of models. This field is massive,
and for my purposes I'll be examining and walking through latent models. Latent models specialize in their ability to understand
complex sorces of data. My research involves the use of multimodal data to improve performance of generative models, so with this
focus we can begin our adventure!

The <b>manifold hypothesis</b> states that all real-world expressions lie upon a smaller subset of data, that
expresses itself in a higher dimensionality. This hypothesis underlies much of how machine learning works, and
intuitively makes sense. When we imagine objects, our brain stores said objects in a collection of neurons, there exists
this compression of information.

Let's say we have a highly complex piece of data x, say a picture of a horse. We wish to compress said horse into a manifold.
The representation that horse resides in is our <b>latent space</b> denoted as z.

What would the process be for not only learning how to represent a horse (autoencoder), but to also represent a <b>distribution</b>
of horses? We want to represent horses, as well as all kinds of horses in a statistical manner. I find a lot of talk
around this topic does the ground work of math first, and builds up to the bigger picture. I'm going to instead describe
the bigger picture, and work my way down to the math such that we cultivate a better intuitive feeling for the math.

[img]

1. We want to encode some x into a latent space z.
2. We want to sample from said latent space z, such that we get a sampling away from out simple sample.
3. We want to decode that sample into a new x'.
4. We wnat to tweak our encoding and decoding process to not only represent x', but to represent the distribution that x' comes from.

Now... 

1. Encode a value to a mean and covariance matrix: $$x \rightarrow \mu(x), \Sigma(x)$$
2. Calculate a sample: $$z = \mu(x) + \Sigma(x) \cdot \epsilon \text{, where } \epsilon \sim \mathcal{N}(0, 1)$$
3. Decode $$z \rightarrow p(x|z) $$
4. Optimize the process, taking into consideration how well the VAE can create horses, as well as deviate from the mean horse in a proper manner.

Let's focus on this last step and how we derive a suitable formula. We know our final optimization must take into
consideration both reconstruction and statistical representation, and we know we can only do this using values we have.
Theoretical values we don't ahve, for instance, is the perfect encoding p(x|z). We don't know what the ideal distribution
is p(z) in a latent space and instead resort to using a gaussian distribution N(0, 1). So let's start from the beginning
and finally derive this equation and see why it makes sense!

The below starting point shows how a value x is a summation of all of its points, as well as the distribution z underneath it.
From our example above, the probability of a certain picture of a horse depends on how likely it is given the distribution.
We'll slowly make this <b>tractable</b>, or able to be calculated. But we'll have to make some substitutions coming up.

$$ p(x) = \int p(x|z)d(z)dz $$

Let's first multiply each side by the natural log, as to simplify our calculation eventually to a summation.

$$ ln(p(x)) = ln \int p(x|z)p(z)dz $$

We also know that we don't have the exact distribution p(z) that perfectly maps onto our latent space, so we'll instead use
a tractable distribution q(z). This borrows from <b>variational bayesian methods</b> which look to provide an approximation
of a previously intractable distribution. For now, just imagine we'll sloppily use the normal distribution N(0, 1) for q(z).

$$ ln(p(x)) = ln \int \frac{q(z)}{q(z)} p(x|z)p(z)dz $$

$$ ln(p(x)) = ln \mathbb{E}_{q(z)}[\frac{p(x|z)p(z)}{q(z)}] \text{, where }\ \mathbb{E} _{q(z)} = \int f(z)q(z)dz $$

We have to move this natural log inside of our expectation, which we can only do with <b>Jensen's Inequality</b>. Jensen's Inequality is used in this case to show that we cannot perfectly optimize this problem, but rather come to an approximation.

$$  $$
