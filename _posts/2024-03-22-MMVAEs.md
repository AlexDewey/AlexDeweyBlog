---
title: "The Theoretical Underpinnings to Analysis of Modern Multimodal Generative Latent Models"
date: 2024-03-22
---

"How do we take in complicated data and accurately generate more of it?"

What motivates this post is an interest and desire to develop a keen understanding of how deep generative models work. From a
theoretical to a practical analysis I want to be able to understand and explain these collections of models. This field is massive,
and for my purposes I'll be examining and walking through latent models. Latent models specialize in their ability to understand
complex sources of data. My research involves the use of multimodal data to improve performance of generative models, so with this
focus we can begin our adventure!

The <b>manifold hypothesis</b> states that all real-world expressions lie upon a smaller subset of data, that
expresses itself in a higher dimensionality. This hypothesis underlies much of how machine learning works, and
intuitively makes sense. When we imagine objects, our brain stores said objects in a collection of neurons, there exists
this compression of information.

Let's say we have a highly complex piece of data x, say a picture of a horse. We wish to compress said horse into a manifold.
The representation that horse resides in is our <b>latent space</b> denoted as z.

What would the process be for not only learning how to represent a horse (autoencoder), but to also represent a <b>distribution</b>
of horses? We want to represent horses, as well as all kinds of horses in a statistical manner. I find a lot of talk
around this topic does the ground work of math first, and builds up to the bigger picture. I'm going to instead describe
the bigger picture, and work my way down to the math such that we cultivate a better intuitive feeling for the math.

![Alt text](https://raw.githubusercontent.com/AlexDewey/AlexDeweyBlog/main/_posts/images/MultiModalVAE/1.png)

1. We want to encode some x into a latent space z. The probability density function (pdf) of this is "p(z\|x)". (A pdf is just a function of probability over a space. So over a discrete space of a coin flip, tails is 0.5 and heads is 0.5. Over a continuous space, we find that calculating the probability at any given space is hard, unless we're dealing with very specific functions be it gaussian or poisson distributions.)
2. We want to sample from said latent space z, such that we get a sampling away from out simple sample. The pdf of this is "p(z)".
3. We want to decode that sample into a new x'. The pdf of this is "p(x'\|z)".
4. We want to tweak our encoding and decoding process to not only represent x', but to represent the distribution that x' comes from.

Now... 

1. Encode a value to a mean and covariance matrix: $$x \rightarrow \mu(x), \Sigma(x)$$
2. Calculate a sample: $$z = \mu(x) + \Sigma(x) \cdot \epsilon \text{, where } \epsilon \sim \mathcal{N}(0, 1)$$
3. Decode $$z \rightarrow p(x\vert z) $$
4. Optimize the process, taking into consideration how well the VAE can create horses, as well as deviate from the mean horse in a proper manner. We are optimizing some parameters given an input x and a chosen prior z.

Let's focus on this last step and how we derive a suitable formula. We know our final optimization must take into
consideration both reconstruction and statistical representation, and we know we can only do this using values we have.
Theoretical values we don't have, for instance, is the perfect encoding p(x|z). We don't know what the ideal distribution
is p(z) in a latent space and instead resort to using a gaussian distribution N(0, 1). So let's start from the beginning
and finally derive this equation and see why it makes sense!

The below equation is our starting point that describes how the probability of x is the probability of x given all
distributions z. Understanding what the actual latent distribution is for z isn't <b>tractable</b>, or isn't really easily
calculable. Instead we'll come up with neat substitutions down the road!

$$ p(x) = \int p(x|z)d(z)dz $$

Let's first multiply each side by the natural log, as to simplify our calculation eventually to a summation.

$$ ln(p(x)) = ln \int p(x|z)p(z)dz $$

We also know that we don't have the exact distribution p(z) that perfectly maps onto our latent space, so we'll instead use
a tractable distribution q(z). This borrows from <b>variational bayesian methods</b> which look to provide an approximation
of a previously intractable distribution. For now, just imagine we'll sloppily use the normal distribution N(0, 1) for q(z).

$$ ln(p(x)) = ln \int \frac{q(z)}{q(z)} p(x|z)p(z)dz $$

$$ ln(p(x)) = ln \mathbb{E}_{q(z|x)}[\frac{p(x|z)p(z)}{q(z)}] \text{, where }\ \mathbb{E} _{q(z|x)} = \int f(z)q(z)dz $$

We have to move this natural log inside of our expectation, which we can only do with <b>Jensen's Inequality</b>. Jensen's
Inequality is used in this case to show that we cannot perfectly optimize this problem, but rather come to an approximation.

![Alt text](https://raw.githubusercontent.com/AlexDewey/AlexDeweyBlog/main/_posts/images/MultiModalVAE/2.png)

Jensen's inequality does mean that we lose some specificity in our approximations, however consider that our optimization
landscape may actually become easier and less noisy, and there's research to show that getting as close to the lower bound of 
error in estimating our distributions may not actually give us the best results.

$$ ln(p(x)) \geq \mathbb{E}_{q(z|x)}[\frac{p(x|z)p(z)}{q(z)}] $$

$$ ln(p(x)) \geq \mathbb{E}_{q(z|x)}[ln(p(x|z)) + ln(p(z)) - ln(q(z))]$$

$$ ln(p(x)) \geq \mathbb{E} _{q(z|x)}[ln(p(x|z))] - \mathbb{E} _{q(z|x)}[ln((q(z)) - ln(p(z))]$$

This equation is known as our Estimated Lower BOund (<b>ELBO</b>). Let's take a step back from this complex formula that isn't
intuitive and think about what exactly we'd want in a formula for generating statistically representative samples. Well, we'd
want a term to punish how accurate the final result. The second term we'd want is one that enforced the statistical
distribution we're looking for. We're basically desiring a mix between complete recreation accuracy and the variation that
we desire in a generative model. This is exactly what we end up with! Let's take a look.

Our first term is the log likelihood of how well our decoder recreates our data x from our latent data z. Our second term is the 
<b>Kullback Leibler Divergence (KLD)</b> equation, in which the statistical distance between one probability distribution from another. Our two distribution is the 
one we create compared to the one we desired.

$$ \mathbb{E} _{q(z|x)}[ln((q(z)) - ln(p(z))] = \text{D} _{\text{KL}}(q(z)\parallel p(z)) = \sum _{Z \epsilon z} q(z) log(\frac{q(z)}{p(z)})$$

Here we're summing up the ratios multiplied by q(z) to find the surprise of how close our distributions align. This way we can
enforce how well our model fits the distribution!

$$ \text{Log Loss} \geq \text{Reconstruction Error} + \text{Statistical Distribution Error} $$

The log of p(x\|z) we take the natural log of the pdf of a gaussian distribution

$$ \text{Reconstruction Error} = \text{MSE}(x, x') = \sum^{n}_{i=1}\frac{(x _{i} - x' _{i})^{2}}{n} $$

To find out the KLD equation for easily comparing a diagonal multivariate normal to a standard
normal distribution I looked it up on wikipedia and found this equation haha.

$$ \text{Statistical Distribution Error} = D_{KL}(\mathcal{N}(\mu(x), \sigma(x)) \parallel \mathcal{N}(0, I)) = \frac{1}{2} \sum^{n} _{i=1}(\sigma^{2} _{i} + \mu^{2} _{i} - 1 - \text{ln}(\sigma^{2} _{i}))$$

In our code now, we'll take an image x, encode it to two separate vectors &mu; and &sigma; in our latent space. We'll sample from said latent space z = &mu; + 
&sigma; * N(0, I). From this z, we pass it into our decoder and get x'. Then we plug in our values (x, x', &mu;, &sigma;) into our equation and get a loss to then 
optimize our parameters for.

$$ \text{Log Loss} \geq \sum^{n}_{i=1}\frac{(x _{i} - x' _{i})^{2}}{n} + \frac{1}{2} \sum^{n} _{i=1}(\sigma^{2} _{i} + \mu^{2} _{i} - 1 - \text{ln}(\sigma^{2} _{i}))$$

I hope this equation makes sense now and where and why we use it!

## Potential Issues ##

<b>Posterior collapse</b> could occur where the proper representation doesn't exist because the encoder and decoder are too powerful. All deviations in our latent space are instead treated as noise and there's no meaningful distinction in our sampling of z.

The <b>Hole Problem</b> is where there is a mismatch between the areas the prior thinks are high probability and that of the posterior encoder think are high probability.

The <b>out-of-distribution problem</b> talks about the difficulties of how a model can understand data it's train on, but then cannot properly generalize or recognize new data from a different, new distribution.

## Improving Our Model ##

![Images from book "Deep Generative Modeling"](https://raw.githubusercontent.com/AlexDewey/AlexDeweyBlog/main/_posts/images/MultiModalVAE/3.png)

As mentioned before, if there's a big mismatch between the aggregated posterior and the prior, we won't be able to represent data properly. Encoding a VAE into a 2d space and filling in the spaces we can see that some areas aren't used by the encoder. This is that mismatch. Instead we can use a <b>Mixture of Gaussians</b> (MoG) prior to better fit the data.

![Alt text](https://raw.githubusercontent.com/AlexDewey/AlexDeweyBlog/main/_posts/images/MultiModalVAE/4.png)

$$ p(z) = \sum^{K} _{k=1}w _{k} \mathcal{N} (z \vert \mu _{k}, \sigma^{2} _{k}) $$

An improvement upon MoG was that of <b>VampPrior: Variational Mixture of Posterior Prior</b>. In the equation below u is a set of trainable parameters that are randomly initialized and trained via backpropagation. What we're doing is using different density estimators to map onto our low-dimensional latent space.

![Alt text](https://raw.githubusercontent.com/AlexDewey/AlexDeweyBlog/main/_posts/images/MultiModalVAE/5.png)

$$ p(z) = \frac{1}{N} \sum^{K} _{k=1} q(z \vert u _{k}) $$

<b>General Topographic Mapping</b> (GTM) defines a grid of K points in a low-dimensional space similar to a sheet a paper that we then scrunching up and wrinkling it to better fit the densities of our actual distribution. We can even combine GTM and VampPrior to get even more complex fits! But remember that complexity isn't always better, as we risk overfitting.

![Alt text](https://raw.githubusercontent.com/AlexDewey/AlexDeweyBlog/main/_posts/images/MultiModalVAE/6.png)

## Flows ##

One promising area that has recently gained popularity is that of flow-VAEs. Let's say we want to model some complex probability distribution. One way of doing so is doing a series of simple transformations on a simple Gaussian until the result roughly approximates our desired probability distribution. Let f be a series of invertible transformations, resulting in a final bijective transformation where every input has exactly one output.

Figuring out the density seems easy. We just use the inverse. However the inverse doesn't take into consideration that our probability density should integrate to 1.

$$ p(x) = p( \mathcal{f}^{-1}(x)) \begin{vmatrix} \text{det} (\frac{\partial \mathcal{f}^{-1}(x)}{\partial x}) \end{vmatrix} $$

The inverse transformation is multiplied by the inverse jacobian determinant. This makes sense given the definition of a jacobian determinant is a scaling factor between one coordinate space and another. We are swapping coordinate systems and need to compensate for the change in density. We do this by mapping out the changes (jacobian), and then finding how much those scale the coordinate system (determinant).

![Alt text](https://raw.githubusercontent.com/AlexDewey/AlexDeweyBlog/main/_posts/images/MultiModalVAE/7.png)

You can probably tell by now, estimating a complex distribution through transformations is incredibly valuable in our latent dimension for variational autoencoders! This is the last prior we examine here, that of a flow-based prior.

Flows were originally used given the image input x, but this was computationally very expensive. Using them on a smaller latent space is much more efficient, and similar to GTM-VampPrior, the hyperparameters should be used to limit the model, as otherwise it may overfit the data.

There is also more theory regarding different types of flows and how to arrive at them, but for the sake of brevity in an already incredibly long post, let's skip this haha.

## Where Do We Go From Here? ##

This is a hard question. In the rapid world of ML, finding what works best may change one year to another. With a constantly developing assortment of architectures, terminology and metrics to test with, what may seem impossible one day is trivial the next. Let's get a better grasp of recent developments related to what we originally came here to find, we're here for statistically-aware multimodal imputation after all! We can go down every rabbit hole, but we must focus on our main pursuit!
