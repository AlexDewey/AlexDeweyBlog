---
title: "I Made My Own Card Game, Then Proceeded To Lose To My Own AI: Reinforcement Learning and TFAgents"
date: 2023-03-30
---

# My cousin and I made a card game called Kerdu along time ago. He was always better than me, so I made an AI to beat him. #

## **Game Rules:** ##

1. Each player has four rows (1-4).
2. At the end of every round, a card will move to the next row (ex: 4->3 or 2->1).
3. If a card is in one of a player's rows, they may play a card of equal or greater value, and discard both cards into a discard pile.
4. A player must have no cards in their first row at the end of a round or they lose.
5. Once both players pass, a round is over and both players may draw up to a handsize of 5 cards.
6. Once the deck is empty, shuffle the discard pile as a new deck.
7. You may attack a player with a card and depending on the value of the card, it gets placed in a different row. (1st Row: 2-3, 2nd Row: 4-10, 3rd Row: Jack, Queen, King, 4th Row: Ace)

## **A balance between aggression and defence:** ##

### The case for aggressive play:  <br />
Because both players will get the same average value of cards, the one who is attacking will always have an advantage. If I'm being attacked with a 9 in my first row, and in my hand I have a 2, 3, 7, Queen and King, then I'll have to use a Queen to beat a 9. This means it's almost always best to attack. What counters this type of play?

### The case for defensive play:  <br />
If a player attacks with high value cards such as Queens Kings and Aces, then if they're attacked with a high value card, they won't be able to defend against it.

Also if I'm too aggressive in attacking, or are playing a lot of cards in a single turn I may run out of cards to play in a round. If then, my opponent plays a 2, I'd have to immediately beat the card, but since I have no cards to play I'd instantly lose. This can be taken to cases where I have 3 cards in my hand, but if my opponent has 4 cards that must be immediately beaten, I'd lose as I cannot discard of all 4 cards with only 3.

## **Hardcoding AI:** ##

For my aggressive, hard to beat hardcoded AI, it would hold onto its highest value cards to defend against Aces and the like, and use all of its low value cards to attack. It would never go under 2-3 cards unless it was forced to defend.

This is a good balance between aggressive and defensive play. We are forcing our opponent to make unsavory trades, we can defend against strong threats easily, and we're not easily suseptible to being barraged with low-level threats we cannot deal with. This is a formitable AI that even a perfect player cannot always beat.

## **Understanding Theory:** ##

### Markov Decision Processes (MDPs): ###
We will first find a base-level representation of our game. We can use MDPs to represent the following:
Agent: Our player. <br />
Environment: Our simple game board and hand. <br />
State: The specific environment our player finds themselves in. This will give us the details of our hand, and what card is in which row. <br />
Action: The move that the player is legally allowed to make. <br />
Reward: The assessment of how good a state is given a particular action. <br />

We want our agent to maximize rewards by analyzing a state and chosing the correct aciton. Everytime an action is taken, there is a subsiquent feedback in the form of a reward.

Our agent's brain is their policy. A policy &pi; defines the level of curiosity we have in exploring our options. Initially we may want to take random actions regardless of how dumb they seem, but slowly over time we want to instead use a probability distribution that narrows down to what actions give us the best results. This is known as an epsilon greedy strategy; a way we can balance exploration with exploitation.

So how do we assess how valuable a state is? We need two functions:
A state-value function $v_{\pi}(s)$ tells us how valuable a given state is. An action-value function $q_{\pi}(s,a)$ tells us how valuable an action is given our current state. These functions are the summations of the expected value given a specific policy.

$$q_{\pi}(s, a) = E[R_{t} | S_{t} = s, A_{t} = a]$$ 

To balance short term losses with long term gains we'll introduce the idea that we want to slowly overtime discount what rewards we can expect to get. If we care about the future just as much as the present we'd set a value gamma (&gamma;) to 1. If we don't care about the future at all we'd set &gamma; to 0 such that our equation below is summing the discounted rewards R<sub>total</sub> at time t.

$$R_{total} = \sum\limits_{k=0}^{\infty} {\gamma}^k * R_{k + t + 1}$$

The following equation therefore states that for given a state and action, the expected reward will be the sumation of all discounted rewards. This is what's known as our Q-function that calculates our Q-value.

$$q_{\pi}(s, a) = E[\sum\limits_{k=0}^{\infty} {\gamma}^k * R_{k + t + 1} | S_{t} = s, A_{t} = a]$$

Q-tables are brought up as a way of storing the value of a particular action with a particular state, however this obviously won't scale well with more complex games with nearly infinite state-action pairs.

The only thing we've been vague on so far is what exactly is the best policy to select. We'd normally use Bellman's Optimality Equation that states that the best policy is whatever maximizes the Q-Value.

$$q_{\star}(s,a) = E[R_{t+1} + \gamma $$

Test
