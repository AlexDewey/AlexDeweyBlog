---
title: "Simple Matrix Factorization Guide"
date: 2023-08-10
---

In trying to learn about ALS Matrix Factorization I realized there wasn't any easy intuitive explanation. So I made this one!

We have a rating matrix R and want to predict unknown ratings.

R:{nxm}, where every row is a user u and every column is an item i. Every entry then corresponds to a rating between user and item r<sub>ui</sub>.

Each u,i gets a k dimensional vector respectively called x_u, y_i.

These k dimensional vectors are called factors.

Predicting an unknown r_ui = x_u.T * y_i. R = X.T * Y | X = [x_1, ... x_n], Y = [y_1, ... y_m]

Finding optimal X and Y could be done using MSE and regularization:

min(X,Y) sum((r_ui-x_u.T*y_i)^2 + lambda(sum(||x_u||^2) + sum(||y_i||^2))

But this is nonconvex and NP hard. Gradient descent is suboptimal.

ALS fixes x and then solves for y, then fixes y and solves for x.

for u=1...n do
x = (sum(y_i*y_i.T + lambda*I_k))^-1 * sum(r_ui*y_i)
end for

The same is for y, just replace all x with y and u with i.

Then to predict, r_ui = x_u.T * y_i for all u, i.,

In calculations we have to calculate an A and B as shown below: x_u = (A + lambda*I_k)^-1 * B
where A is the y_i*y_i.T for every rated item in u.

B is the rating*y_i for every rating item in u.
